#!/usr/bin/env python3
"""
Script de teste para validar o sistema Shell Assistant
"""

import json
import requests
import subprocess
import sys
from typing import Dict, List, Tuple

def test_ollama_connection() -> bool:
    """Testa conexÃ£o com Ollama"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        return response.status_code == 200
    except:
        return False

def test_model_availability(model: str) -> bool:
    """Testa se o modelo estÃ¡ disponÃ­vel"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            return any(m.get("name", "").startswith(model) for m in models)
        return False
    except:
        return False

def test_command_generation(prompt: str, expected_command: str, model: str = "llama3.2:latest") -> Tuple[bool, str]:
    """Testa geraÃ§Ã£o de comando"""
    url = "http://localhost:11434/api/chat"
    
    payload = {
        "model": model,
        "messages": [
            {
                "role": "system",
                "content": "VocÃª Ã© um assistente especializado em comandos shell Linux. Sua funÃ§Ã£o Ã© interpretar comandos em linguagem natural e retornar APENAS o comando shell correspondente, sem explicaÃ§Ãµes, comentÃ¡rios ou formataÃ§Ã£o adicional.\n\nRegras:\n- Retorne apenas o comando shell puro\n- NÃ£o adicione explicaÃ§Ãµes ou comentÃ¡rios\n- NÃ£o use markdown ou formataÃ§Ã£o\n- Use comandos padrÃ£o do Linux\n- Considere o contexto: usuÃ¡rio root, diretÃ³rio atual, variÃ¡veis de ambiente\n- Se mÃºltiplos comandos forem necessÃ¡rios, separe-os com '&&' ou ';'\n- Para comandos que requerem privilÃ©gios, use 'sudo' quando apropriado"
            },
            {
                "role": "user",
                "content": prompt
            }
        ],
        "max_tokens": 2000,
        "temperature": 0.1,
        "stream": False
    }
    
    try:
        response = requests.post(url, json=payload, timeout=30)
        if response.status_code == 200:
            result = response.json()
            generated_command = result['message']['content'].strip()
            
            # ComparaÃ§Ã£o flexÃ­vel - ignora espaÃ§os extras e diferenÃ§as menores
            generated_clean = generated_command.replace("  ", " ").strip()
            expected_clean = expected_command.replace("  ", " ").strip()
            
            success = generated_clean == expected_clean
            return success, generated_command
        else:
            return False, f"Erro HTTP: {response.status_code}"
    except Exception as e:
        return False, f"Erro: {str(e)}"

def test_safe_command_execution(command: str) -> Tuple[bool, str]:
    """Testa execuÃ§Ã£o segura de comando (apenas comandos de leitura)"""
    safe_commands = [
        "ps aux", "df -h", "free -h", "ls -la", "uname -a", 
        "whoami", "pwd", "echo 'test'", "date", "uptime"
    ]
    
    # Verifica se Ã© um comando seguro
    command_base = command.split()[0] if command else ""
    is_safe = any(safe in command for safe in safe_commands)
    
    if not is_safe:
        return False, "Comando nÃ£o Ã© seguro para teste automÃ¡tico"
    
    try:
        result = subprocess.run(command, shell=True, capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            return True, result.stdout[:200] + "..." if len(result.stdout) > 200 else result.stdout
        else:
            return False, result.stderr
    except subprocess.TimeoutExpired:
        return False, "Timeout na execuÃ§Ã£o"
    except Exception as e:
        return False, f"Erro: {str(e)}"

def run_tests():
    """Executa todos os testes"""
    print("=== Teste do Sistema Shell Assistant ===\n")
    
    # Teste 1: ConexÃ£o com Ollama
    print("1. Testando conexÃ£o com Ollama...")
    if test_ollama_connection():
        print("   âœ… Ollama estÃ¡ rodando")
    else:
        print("   âŒ Ollama nÃ£o estÃ¡ acessÃ­vel")
        print("   Execute: ollama serve")
        return False
    
    # Teste 2: Disponibilidade do modelo
    print("\n2. Testando disponibilidade do modelo llama3.2...")
    if test_model_availability("llama3.2"):
        print("   âœ… Modelo llama3.2 disponÃ­vel")
    else:
        print("   âŒ Modelo llama3.2 nÃ£o encontrado")
        print("   Execute: ollama pull llama3.2:latest")
        return False
    
    # Teste 3: GeraÃ§Ã£o de comandos
    print("\n3. Testando geraÃ§Ã£o de comandos...")
    
    test_cases = [
        ("liste os processos em execuÃ§Ã£o", "ps aux"),
        ("mostre o espaÃ§o em disco", "df -h"),
        ("mostre o uso de memÃ³ria", "free -h"),
        ("crie um arquivo teste.txt", "touch teste.txt"),
        ("mostre informaÃ§Ãµes do sistema", "uname -a")
    ]
    
    passed_tests = 0
    total_tests = len(test_cases)
    
    for prompt, expected in test_cases:
        print(f"   Testando: '{prompt}'")
        success, result = test_command_generation(prompt, expected)
        
        if success:
            print(f"   âœ… Gerado: '{result}'")
            passed_tests += 1
        else:
            print(f"   âŒ Esperado: '{expected}', Gerado: '{result}'")
    
    print(f"\n   Resultado: {passed_tests}/{total_tests} testes passaram")
    
    # Teste 4: ExecuÃ§Ã£o segura de comandos
    print("\n4. Testando execuÃ§Ã£o segura de comandos...")
    
    safe_test_cases = [
        ("ps aux", "Lista de processos"),
        ("df -h", "EspaÃ§o em disco"),
        ("free -h", "Uso de memÃ³ria"),
        ("uname -a", "InformaÃ§Ãµes do sistema")
    ]
    
    safe_passed = 0
    safe_total = len(safe_test_cases)
    
    for command, description in safe_test_cases:
        print(f"   Testando: {description} ({command})")
        success, output = test_safe_command_execution(command)
        
        if success:
            print(f"   âœ… Executado com sucesso")
            safe_passed += 1
        else:
            print(f"   âŒ Falha: {output}")
    
    print(f"\n   Resultado: {safe_passed}/{safe_total} comandos executados com sucesso")
    
    # Resumo final
    print("\n=== Resumo dos Testes ===")
    print(f"âœ… ConexÃ£o Ollama: OK")
    print(f"âœ… Modelo disponÃ­vel: OK")
    print(f"âœ… GeraÃ§Ã£o de comandos: {passed_tests}/{total_tests}")
    print(f"âœ… ExecuÃ§Ã£o segura: {safe_passed}/{safe_total}")
    
    overall_success = passed_tests >= total_tests * 0.8 and safe_passed >= safe_total * 0.8
    
    if overall_success:
        print("\nğŸ‰ Sistema funcionando corretamente!")
        return True
    else:
        print("\nâš ï¸  Sistema com problemas. Verifique os erros acima.")
        return False

def test_specific_command():
    """Testa um comando especÃ­fico fornecido pelo usuÃ¡rio"""
    if len(sys.argv) < 3:
        print("Uso: python3 test_system.py test <comando em linguagem natural>")
        return
    
    prompt = " ".join(sys.argv[2:])
    print(f"Testando comando: '{prompt}'")
    
    success, result = test_command_generation(prompt, "")
    if success:
        print(f"âœ… Comando gerado: '{result}'")
        
        # Testa execuÃ§Ã£o se for seguro
        exec_success, exec_output = test_safe_command_execution(result)
        if exec_success:
            print(f"âœ… Executado com sucesso")
            print(f"SaÃ­da: {exec_output}")
        else:
            print(f"âš ï¸  NÃ£o foi possÃ­vel executar automaticamente: {exec_output}")
    else:
        print(f"âŒ Erro na geraÃ§Ã£o: {result}")

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        test_specific_command()
    else:
        run_tests()