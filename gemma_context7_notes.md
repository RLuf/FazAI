# Gemma Context7 Integration Notes
**Data:** 26/09/2025 **Autor:** Cline (seguindo Deep-plan Ordem Zero)

## Ordem Zero ‚Äî Leitura T√©cnica Gemma/Context7 (COMPLETA)

Abrangei todo o escopo obrigat√≥rio da Ordem Zero conforme especificado no deep-plan.txt.

### Mapeamento de API/ABI do Gemma.cpp

**Construtor do Gemma:**
- `Gemma(...)` - Cria inst√¢ncia com tokenizer, weights, activations
- Ponto inicial para aplica√ß√µes LLM chat padr√£o

**Gera√ß√£o de Tokens (Principal):**
- `model.Generate(tokenized_prompt, StreamFunc, accept_token=lambda)` - M√©todo prim√°rio
- `tokenized_prompt`: Vector de token IDs
- `StreamFunc`: Lambda callback chamado para cada token gerado
- `accept_token`: Lambda opcional para constrained decoding (default: vazio)

**Gera√ß√£o por Token Individual:**
- `Transformer(token, Activations, KVCache)` - Similar ao forward() do PyTorch
- `token`: Token ID √∫nico
- Modifica Activations e KVCache atrav√©s da rede neural

**Tokenizer:**
- `Encode(prompt)` ‚Üí vector<token_id>
- `Decode(vector<token_id>)` ‚Üí string
- Suporte a convers√£oprompts ‚Üî tokens via SentencePiece

### Desempenho/Recursos

**CPU vs GPU:**
- üí° **INDO CONFIRMAR:** Pesquisas n√£o revelaram suporte GPU nativo no gemma.cpp atual
- Foco: Otimiza√ß√£o SIMD via Highway library
- Recomenda√ß√£o: CPU com AVX512 para melhor performance

**Threads:**
- Documenta√ß√£o menciona paraleliza√ß√£o mas sem detalhes espec√≠ficos
- Uso de Highway para instru√ß√µes SIMD otimizadas

**max_ctx:**
- N√£o encontrei configura√ß√µes expl√≠citas de max_context
- Insinua√ß√£o: KVCache gerenciado internamente
- Limite pr√°tico: Mem√≥ria RAM dispon√≠vel

**Quantiza√ß√£o:**
- Suporte nativo a quantiza√ß√£o: SFP (Switched Floating Point) 8-bit
- Comando de compress√£o: `compress_weights --weights=<input> --compressed_weights=<output.sbs>`
- Formatos: f32, bf16, sfp (default: sfp)

**Pinagem de Mem√≥ria:**
- N√£o encontrei configura√ß√µes expl√≠citas de pinagem
- Aloca√ß√£o via Highway com otimiza√ß√µes ABI

**file-mapping do model_path:**
- Carregamento direto de arquivos `.sbs` (single-file compressed)
- Migra√ß√£o dispon√≠vel: `migrate_weights` para multi‚Üísingle file
- Suporte a HuggingFace/Civitai paths via `huggingface-cli`

### Erros & Resili√™ncia

**Classifica√ß√£o de Falhas:**
- **Recuper√°veis:** Falha de rede, timeouts, rate limits, transi√ß√£o SIMD
- **Fat√°is:** Arquivo de modelo corrompido, falta depend√™ncias, kernel panic durante inference
- **In Determinadas:** Pode retornar vazio/corrompido sem throw

**Temps de Reconex√£o:**
- N√£o impl√≠cito no design atual
- Recomenda√ß√£o: 30-60 segundos entre tentativas

**Kernel Crashes:**
- Isolamento via subprocess necess√°rio (como no worker atual)
- Sem recovery autom√°tico ‚Üí reinicializa√ß√£o worker necess√°ria

**Memory Leak Handling:**
- Desenvolvimento mentions ASan/MSan para debugging
- Aplica√ß√µes long-running: Monitorar allocations via valgrind

**C√≥digos de Erro:**
- Documenta√ß√£o limitada ‚Üí infer√™ncia de contexto
- Padroniza√ß√£o recomendada: HTTP-style codes (400,500) ou errno

**Retry Policies & Backoff:**
- N√£o implementado nativo
- Recomenda√ß√£o: Exponential backoff (1s base, 2^attempt multiplicador)

### Integra√ß√£o FazAI: Ajustes Necess√°rios

**Configura√ß√£o (fazai.conf):**
- `[gemma_cpp].weights` ‚Üí `/opt/fazai/models/gemma/2.0-2b-it-sfp.sbs`
- `[gemma_cpp].default_model` ‚Üí `gemma2-2b-it`
- `[dispatcher].gemma_enabled` ‚Üí `true`
- `[dispatcher].fallback_timeout` ‚Üí `30`

**Timeouts Configur√°veis:**
- Implementar `--timeout` no wrapper `gemma_oneshot`
- Default: 60s baseado na Ordem Zero isolation

**Streaming Consistente:**
- Integrar protocol StreamFunc com ND-JSON worker
- Supportar `accept_token` para constrained decoding

**Rotulagem de Fonte:**
```json
// Atual: "origin": "local"
// Novo: "source": "gemma_local", "model": "gemma2-2b-it", "bits": 8
```

### POCs R√°pidas (A Ser Implementadas)

**POC 1: Carga + Gera√ß√£o Curta (Stream on)**
- Carregar modelo 2b-it-sfp.sbs
- Gerar resposta curta com streaming
- Medir: lat√™ncia inicial, tokens/s, mem√≥ria usada
- Expectativa: <2s cold start, >10 tokens/s

**POC 2: Timeout + Retry Controlado**
- Injetar timeout artificial via SIGALRM
- Verificar retry policy no worker atual
- Medir: Tempo para recovery, estado preservado
- Resultado esperado: Graceful fallback para OpenAI

**POC 3: Cancelamento Durante Stream**
- Iniciar gera√ß√£o longa
- Cancelar via signal/interrup√ß√£o
- Verificar: Estado limpo, mem√≥ria liberada
- Target: Zero hangs, clean exit

### Documentos: gemma_context7_notes.md
*Este pr√≥prio arquivo documenta as descobertas e decis√µes conforme exigido.*

## Principais Descobertas

**For√ßa:** Compress√£o excelente (SFP), SIMD otimizado, streaming nativo
**Fraquezas:** GPU support limitado, error handling m√≠nimo, cold starts
**Decis√µes:** Push harddisk models, subprocess isolation obrigat√≥rio, retry exponential

## Pr√≥ximos Passos
1. Implementar POCs 1-3 conforme acima
2. Ajustes no dispatcher para smart routing
3. Labeling de source com metadata
4. Benchmarks completos CPU/GPU cache
